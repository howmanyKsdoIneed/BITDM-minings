{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b2b259",
   "metadata": {},
   "source": [
    "# 互评作业2：频繁模式与关联规则\n",
    "\n",
    "运行此notebook需要使用matplotlib库。\n",
    "\n",
    "## 0 数据集简介及任务说明\n",
    "\n",
    "本次互评作业使用的数据集是Chicago Building Violations。本数据集包含芝加哥从2006年到2019年所开具的建筑违规罚单。这些罚单记录了罚单的ID、产生日期、最后修改日期、违规的具体情形、建筑地址等等信息。数据集中记录了每一列的意义，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9100ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "fjson = open(\"Chicago Building Violations/socrata_metadata.json\")\n",
    "\n",
    "json_data=json.load(fjson)\n",
    "print('#\\tcolumn')\n",
    "for column in json_data['columns']:\n",
    "    print(column['position']-1,end='\\t')\n",
    "    print(column['name'])\n",
    "    if 'description' in column:\n",
    "        print('\\t\\t'+column['description'])\n",
    "        \n",
    "# 数据集有些大，进行一些清理工作以节约内存\n",
    "del json_data\n",
    "fjson.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e29016",
   "metadata": {},
   "source": [
    "接下来介绍我们讨论的主要问题。我们先统计记录的个数以及涉及房屋的数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    with open(filename) as f:\n",
    "        csv_content = csv.reader(f)\n",
    "        columns = next(csv_content)\n",
    "        rows = [row for row in csv_content]\n",
    "    return columns, rows\n",
    "\n",
    "# 对标称值计数以及画条形图\n",
    "def count_occurence(data,column,ignore_empty=True):\n",
    "    '''统计数据某个特定列中所有元素的出现次数，返回一个dict-like\n",
    "    如果置ignore-empty为True，则返回的字典中不包含空值的计数'''\n",
    "    count = defaultdict(int)\n",
    "    for row in data:\n",
    "        if ignore_empty and row[column]=='':\n",
    "            continue\n",
    "        count[row[column]]+=1\n",
    "    return count\n",
    "\n",
    "cols_csv,rows_csv = read_csv('Chicago Building Violations/building-violations.csv')\n",
    "print(f'数据集中有{len(rows_csv)}条记录')\n",
    "addr_occurence=count_occurence(rows_csv,16,False)\n",
    "assert '' not in addr_occurence # 显然，数据集中每一条记录都是有对应房屋地址的\n",
    "print(f'数据集涉及{len(addr_occurence)}个房屋地址')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd9b58",
   "metadata": {},
   "source": [
    "可见数据集中涉及房屋地址的数量比罚单数量少一个数量级左右，因此本次互评作业的任务是探究房屋开具罚单（确切地说，罚单类型）的模式和关联关系。\n",
    "\n",
    "## 1 数据的预处理\n",
    "\n",
    "接下来，我们对数据集进行一些预处理，仅保留我们需要的内容。我们想要保留的数据只有两类：1）违规代码与其解释的对应关系；2）每个房屋产生过哪些违规罚单。为了节约内存，我们忽略掉其他部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b021e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 违规代码到对应解释的映射关系\n",
    "viol2code=[]\n",
    "viol2desp=[]\n",
    "code2viol={}\n",
    "# 建筑物到其地址的关系\n",
    "bld2addr=[]\n",
    "addr2bld={}\n",
    "# 建筑物ID到其产生的罚单表的关系\n",
    "bld2viols=[]\n",
    "for row in rows_csv:\n",
    "    # 整理违规代码对应的解释\n",
    "    code = row[3]\n",
    "    desp = row[6]\n",
    "    violid=-1\n",
    "    if code in code2viol:\n",
    "        violid=code2viol[code]\n",
    "    else:\n",
    "        violid = len(viol2code)\n",
    "        viol2code.append(code)\n",
    "        viol2desp.append(desp)\n",
    "        code2viol[code]=violid\n",
    "    # 整理建筑物罚单\n",
    "    addr = row[16]\n",
    "    bldid = -1\n",
    "    if addr in addr2bld:\n",
    "        bldid = addr2bld[addr]\n",
    "    else:\n",
    "        bldid = len(bld2addr)\n",
    "        bld2addr.append(addr)\n",
    "        bld2viols.append([])\n",
    "        addr2bld[addr]=bldid\n",
    "    bld2viols[bldid].append(violid)\n",
    "# 原始数据量很大，故删除之以节省内存\n",
    "del rows_csv\n",
    "del addr2bld\n",
    "del code2viol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079d2c0",
   "metadata": {},
   "source": [
    "我们来看一下每个建筑物对应的罚单数量的分布情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4608fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_count=sorted([len(l)for l in bld2violations])\n",
    "bld_count = len(viol_count)\n",
    "plt.figure()\n",
    "plt.title('Q-chart on violations-per-building')\n",
    "plt.plot(viol_count,[i/bld_count for i in range(bld_count)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3310b",
   "metadata": {},
   "source": [
    "从上述统计看，绝大多数建筑只有很少（不到50个）的罚单，但个别建筑有很多罚单。\n",
    "\n",
    "## 2 频繁模式统计\n",
    "\n",
    "现在我们统计这对关系中的频繁模式。我们使用Apriori方法找到所有的频繁模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support=.01\n",
    "\n",
    "#Apriori算法的原实现来自\n",
    "#https://blog.csdn.net/qq_39872846/article/details/105291265\n",
    "#使用时有改动\n",
    "\n",
    "# 求第一次扫描数据库后的候选集\n",
    "# 就是求这个数据库中出现了几个元素，然后返回\n",
    "def item(dataset):      \n",
    "    c1 = [] # 存放候选集元素\n",
    "\n",
    "    for x in dataset:\n",
    "        for y in x:\n",
    "            if [y] not in c1:\n",
    "                c1.append( [y] )\n",
    "    c1.sort()\n",
    "    return c1\n",
    "\n",
    "# 求第k次候选集\n",
    "def get_candidate(Fk, K):       \n",
    "    ck = []    #存放产生候选集\n",
    "\n",
    "    for i in range(len(Fk)):\n",
    "        for j in range(i+1, len(Fk)):\n",
    "            L1 = list(Fk[i])[:K-2]\n",
    "            L2 = list(Fk[j])[:K-2]\n",
    "            L1.sort()\n",
    "            L2.sort() #先排序，在进行组合\n",
    "\n",
    "            if L1 == L2:\n",
    "                if K > 2:\n",
    "                    new = list(set(Fk[i]) ^ set(Fk[j]) )\n",
    "                else:\n",
    "                    new = set()\n",
    "                for x in Fk: \n",
    "                    #剪枝：new是 x 的子集，并且 还没有加入 ck 中\n",
    "                    if set(new).issubset(set(x)) and list(set(Fk[i]) | set(Fk[j])) not in ck: \n",
    "                        ck.append( list(set(Fk[i]) | set(Fk[j])) )\n",
    "    return ck\n",
    "\n",
    "def get_frequent_item(dataset, c, min_support):\n",
    "    cut_branch = {}     #用来存放所有项集的支持度的字典\n",
    "    for x in c:\n",
    "        for y in dataset:\n",
    "            if set(x).issubset(set(y)):     #如果 x 不在 y中，就把对应元素后面加 1\n",
    "                cut_branch[tuple(x)] = cut_branch.get(tuple(x), 0) + 1\n",
    "                #cut_branch[y] = new_cand.get(y, 0)表示如果字典里面没有想要的关键词，就返回0\n",
    "\n",
    "    Fk = []       #支持度大于最小支持度的项集，  即频繁项集\n",
    "    sup_dataK = {}  #用来存放所有 频繁 项集的支持度的字典\n",
    "\n",
    "    for i in cut_branch:\n",
    "        if cut_branch[i] >= min_support:\n",
    "            Fk.append( list(i))\n",
    "            sup_dataK[i] = cut_branch[i]\n",
    "    return Fk, sup_dataK\n",
    "\n",
    "# 此方法返回一个三维列表，第0维是历次迭代产生的频繁项集，第1维是各个频繁项集，第2维是集合中的各个项\n",
    "def Apriori(dataset, min_support = 2):\n",
    "    c1 = item (dataset) #返回一个二维列表，里面的每一个一维列表，都是第一次候选集的元素\n",
    "    f1, sup_1 = get_frequent_item(dataset, c1, min_support)       #求第一次候选集\n",
    "\n",
    "    F = [f1]      #将第一次候选集产生的频繁项集放入 F ,以后每次扫描产生的所有频繁项集都放入里面\n",
    "    sup_data = sup_1       #一个字典，里面存放所有产生的候选集，及其支持度\n",
    "\n",
    "    K = 2 #从第二个开始循环求解，先求候选集，在求频繁项集\n",
    "\n",
    "    while (len(F[K-2]) > 1):  #k-2是因为F是从0开始数的     #前一个的频繁项集个数在2个或2个以上，才继续循环，否则退出\n",
    "        ck = get_candidate(F[K-2], K)  #求第k次候选集\n",
    "        fk, sup_k = get_frequent_item(dataset, ck, min_support)     #求第k次频繁项集\n",
    "\n",
    "        F.append(fk)    #把新产生的候选集加入F\n",
    "        sup_data.update(sup_k)  #字典更新，加入新得出的数据\n",
    "        K+=1\n",
    "    return F, sup_data    #返回所有频繁项集， 以及存放频繁项集支持度的字典\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
